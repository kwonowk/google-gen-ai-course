{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jkxRSYjzA1oX"
   },
   "source": [
    "##### Copyright 2024 Google LLC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "csNPnkuCobmG"
   },
   "source": [
    "# Prompting\n",
    "\n",
    "This notebook is based on <a href='https://www.kaggle.com/code/markishere/day-1-prompting'>Day 1</a> material of Google's Generative AI course, and <a href = 'https://www.kaggle.com/whitepaper-prompt-engineering'>Google's whitepaper on Prompt Engineering</a> by Lee Boonstra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UAjUV3BsvFXQ"
   },
   "source": [
    "### Install the SDK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Suggested template for documenting prompts\n",
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-14T10:09:52.461181Z",
     "iopub.status.busy": "2024-11-14T10:09:52.460777Z",
     "iopub.status.idle": "2024-11-14T10:10:22.178811Z",
     "shell.execute_reply": "2024-11-14T10:10:22.177337Z",
     "shell.execute_reply.started": "2024-11-14T10:09:52.461142Z"
    },
    "id": "NzwzJFU9LqkJ",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install -U -q \"google-generativeai>=0.8.3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-14T10:10:22.181761Z",
     "iopub.status.busy": "2024-11-14T10:10:22.181376Z",
     "iopub.status.idle": "2024-11-14T10:10:23.390039Z",
     "shell.execute_reply": "2024-11-14T10:10:23.388887Z",
     "shell.execute_reply.started": "2024-11-14T10:10:22.181721Z"
    },
    "id": "5DwxYIRavMST",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "from IPython.display import HTML, Markdown, display\n",
    "from env import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DNEt2BCOvOJ1"
   },
   "source": [
    "### Set up API key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-14T10:10:27.542753Z",
     "iopub.status.busy": "2024-11-14T10:10:27.541960Z",
     "iopub.status.idle": "2024-11-14T10:10:27.750350Z",
     "shell.execute_reply": "2024-11-14T10:10:27.749082Z",
     "shell.execute_reply.started": "2024-11-14T10:10:27.542711Z"
    },
    "id": "SHl0bkPCvayd",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "GOOGLE_API_KEY = GOOGLE_API_KEY\n",
    "genai.configure(api_key=GOOGLE_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List of available models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/chat-bison-001\n",
      "models/text-bison-001\n",
      "models/embedding-gecko-001\n",
      "models/gemini-1.0-pro-latest\n",
      "models/gemini-1.0-pro\n",
      "models/gemini-pro\n",
      "models/gemini-1.0-pro-001\n",
      "models/gemini-1.0-pro-vision-latest\n",
      "models/gemini-pro-vision\n",
      "models/gemini-1.5-pro-latest\n",
      "models/gemini-1.5-pro-001\n",
      "models/gemini-1.5-pro-002\n",
      "models/gemini-1.5-pro\n",
      "models/gemini-1.5-pro-exp-0801\n",
      "models/gemini-1.5-pro-exp-0827\n",
      "models/gemini-1.5-flash-latest\n",
      "models/gemini-1.5-flash-001\n",
      "models/gemini-1.5-flash-001-tuning\n",
      "models/gemini-1.5-flash\n",
      "models/gemini-1.5-flash-exp-0827\n",
      "models/gemini-1.5-flash-002\n",
      "models/gemini-1.5-flash-8b\n",
      "models/gemini-1.5-flash-8b-001\n",
      "models/gemini-1.5-flash-8b-latest\n",
      "models/gemini-1.5-flash-8b-exp-0827\n",
      "models/gemini-1.5-flash-8b-exp-0924\n",
      "models/learnlm-1.5-pro-experimental\n",
      "models/gemini-exp-1114\n",
      "models/gemini-exp-1121\n",
      "models/embedding-001\n",
      "models/text-embedding-004\n",
      "models/aqa\n"
     ]
    }
   ],
   "source": [
    "# Retrieving the names of available models\n",
    "for model in genai.list_models():\n",
    "    print(model.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>version</th>\n",
       "      <th>display_name</th>\n",
       "      <th>description</th>\n",
       "      <th>input_token_limit</th>\n",
       "      <th>output_token_limit</th>\n",
       "      <th>supported_generation_methods</th>\n",
       "      <th>temperature</th>\n",
       "      <th>max_temperature</th>\n",
       "      <th>top_p</th>\n",
       "      <th>top_k</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>'models/chat-bison-001'</td>\n",
       "      <td>'001'</td>\n",
       "      <td>'PaLM 2 Chat (Legacy)'</td>\n",
       "      <td>'A legacy text-only model optimized for chat c...</td>\n",
       "      <td>4096</td>\n",
       "      <td>1024</td>\n",
       "      <td>['generateMessage', 'countMessageTokens']</td>\n",
       "      <td>0.25</td>\n",
       "      <td>None</td>\n",
       "      <td>0.95</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>'models/text-bison-001'</td>\n",
       "      <td>'001'</td>\n",
       "      <td>'PaLM 2 (Legacy)'</td>\n",
       "      <td>'A legacy model that understands text and gene...</td>\n",
       "      <td>8196</td>\n",
       "      <td>1024</td>\n",
       "      <td>['generateText', 'countTextTokens', 'createTun...</td>\n",
       "      <td>0.7</td>\n",
       "      <td>None</td>\n",
       "      <td>0.95</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>'models/embedding-gecko-001'</td>\n",
       "      <td>'001'</td>\n",
       "      <td>'Embedding Gecko'</td>\n",
       "      <td>'Obtain a distributed representation of a text.'</td>\n",
       "      <td>1024</td>\n",
       "      <td>1</td>\n",
       "      <td>['embedText', 'countTextTokens']</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>'models/gemini-1.0-pro-latest'</td>\n",
       "      <td>'001'</td>\n",
       "      <td>'Gemini 1.0 Pro Latest'</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30720</td>\n",
       "      <td>2048</td>\n",
       "      <td>['generateContent', 'countTokens']</td>\n",
       "      <td>0.9</td>\n",
       "      <td>None</td>\n",
       "      <td>1.0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>'models/gemini-1.0-pro'</td>\n",
       "      <td>'001'</td>\n",
       "      <td>'Gemini 1.0 Pro'</td>\n",
       "      <td>'The best model for scaling across a wide rang...</td>\n",
       "      <td>30720</td>\n",
       "      <td>2048</td>\n",
       "      <td>['generateContent', 'countTokens']</td>\n",
       "      <td>0.9</td>\n",
       "      <td>None</td>\n",
       "      <td>1.0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>'models/gemini-pro'</td>\n",
       "      <td>'001'</td>\n",
       "      <td>'Gemini 1.0 Pro'</td>\n",
       "      <td>'The best model for scaling across a wide rang...</td>\n",
       "      <td>30720</td>\n",
       "      <td>2048</td>\n",
       "      <td>['generateContent', 'countTokens']</td>\n",
       "      <td>0.9</td>\n",
       "      <td>None</td>\n",
       "      <td>1.0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>'models/gemini-1.0-pro-001'</td>\n",
       "      <td>'001'</td>\n",
       "      <td>'Gemini 1.0 Pro 001 (Tuning)'</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30720</td>\n",
       "      <td>2048</td>\n",
       "      <td>['generateContent', 'countTokens', 'createTune...</td>\n",
       "      <td>0.9</td>\n",
       "      <td>None</td>\n",
       "      <td>1.0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>'models/gemini-1.0-pro-vision-latest'</td>\n",
       "      <td>'001'</td>\n",
       "      <td>'Gemini 1.0 Pro Vision'</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12288</td>\n",
       "      <td>4096</td>\n",
       "      <td>['generateContent', 'countTokens']</td>\n",
       "      <td>0.4</td>\n",
       "      <td>None</td>\n",
       "      <td>1.0</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>'models/gemini-pro-vision'</td>\n",
       "      <td>'001'</td>\n",
       "      <td>'Gemini 1.0 Pro Vision'</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12288</td>\n",
       "      <td>4096</td>\n",
       "      <td>['generateContent', 'countTokens']</td>\n",
       "      <td>0.4</td>\n",
       "      <td>None</td>\n",
       "      <td>1.0</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>'models/gemini-1.5-pro-latest'</td>\n",
       "      <td>'001'</td>\n",
       "      <td>'Gemini 1.5 Pro Latest'</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2000000</td>\n",
       "      <td>8192</td>\n",
       "      <td>['generateContent', 'countTokens']</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.95</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>'models/gemini-1.5-pro-001'</td>\n",
       "      <td>'001'</td>\n",
       "      <td>'Gemini 1.5 Pro 001'</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2000000</td>\n",
       "      <td>8192</td>\n",
       "      <td>['generateContent', 'countTokens', 'createCach...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.95</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>'models/gemini-1.5-pro-002'</td>\n",
       "      <td>'002'</td>\n",
       "      <td>'Gemini 1.5 Pro 002'</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2000000</td>\n",
       "      <td>8192</td>\n",
       "      <td>['generateContent', 'countTokens', 'createCach...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.95</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>'models/gemini-1.5-pro'</td>\n",
       "      <td>'001'</td>\n",
       "      <td>'Gemini 1.5 Pro'</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2000000</td>\n",
       "      <td>8192</td>\n",
       "      <td>['generateContent', 'countTokens']</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.95</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>'models/gemini-1.5-pro-exp-0801'</td>\n",
       "      <td>'exp-0801'</td>\n",
       "      <td>'Gemini 1.5 Pro Experimental 0801'</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2000000</td>\n",
       "      <td>8192</td>\n",
       "      <td>['generateContent', 'countTokens']</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.95</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>'models/gemini-1.5-pro-exp-0827'</td>\n",
       "      <td>'exp-0827'</td>\n",
       "      <td>'Gemini 1.5 Pro Experimental 0827'</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2000000</td>\n",
       "      <td>8192</td>\n",
       "      <td>['generateContent', 'countTokens']</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.95</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>'models/gemini-1.5-flash-latest'</td>\n",
       "      <td>'001'</td>\n",
       "      <td>'Gemini 1.5 Flash Latest'</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1000000</td>\n",
       "      <td>8192</td>\n",
       "      <td>['generateContent', 'countTokens']</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.95</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>'models/gemini-1.5-flash-001'</td>\n",
       "      <td>'001'</td>\n",
       "      <td>'Gemini 1.5 Flash 001'</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1000000</td>\n",
       "      <td>8192</td>\n",
       "      <td>['generateContent', 'countTokens', 'createCach...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.95</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>'models/gemini-1.5-flash-001-tuning'</td>\n",
       "      <td>'001'</td>\n",
       "      <td>'Gemini 1.5 Flash 001 Tuning'</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16384</td>\n",
       "      <td>8192</td>\n",
       "      <td>['generateContent', 'countTokens', 'createTune...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.95</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>'models/gemini-1.5-flash'</td>\n",
       "      <td>'001'</td>\n",
       "      <td>'Gemini 1.5 Flash'</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1000000</td>\n",
       "      <td>8192</td>\n",
       "      <td>['generateContent', 'countTokens']</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.95</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>'models/gemini-1.5-flash-exp-0827'</td>\n",
       "      <td>'exp-0827'</td>\n",
       "      <td>'Gemini 1.5 Flash Experimental 0827'</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1000000</td>\n",
       "      <td>8192</td>\n",
       "      <td>['generateContent', 'countTokens']</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.95</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>'models/gemini-1.5-flash-002'</td>\n",
       "      <td>'002'</td>\n",
       "      <td>'Gemini 1.5 Flash 002'</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1000000</td>\n",
       "      <td>8192</td>\n",
       "      <td>['generateContent', 'countTokens', 'createCach...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.95</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>'models/gemini-1.5-flash-8b'</td>\n",
       "      <td>'001'</td>\n",
       "      <td>'Gemini 1.5 Flash-8B'</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1000000</td>\n",
       "      <td>8192</td>\n",
       "      <td>['createCachedContent', 'generateContent', 'co...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.95</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>'models/gemini-1.5-flash-8b-001'</td>\n",
       "      <td>'001'</td>\n",
       "      <td>'Gemini 1.5 Flash-8B 001'</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1000000</td>\n",
       "      <td>8192</td>\n",
       "      <td>['createCachedContent', 'generateContent', 'co...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.95</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>'models/gemini-1.5-flash-8b-latest'</td>\n",
       "      <td>'001'</td>\n",
       "      <td>'Gemini 1.5 Flash-8B Latest'</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1000000</td>\n",
       "      <td>8192</td>\n",
       "      <td>['createCachedContent', 'generateContent', 'co...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.95</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>'models/gemini-1.5-flash-8b-exp-0827'</td>\n",
       "      <td>'001'</td>\n",
       "      <td>'Gemini 1.5 Flash 8B Experimental 0827'</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1000000</td>\n",
       "      <td>8192</td>\n",
       "      <td>['generateContent', 'countTokens']</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.95</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>'models/gemini-1.5-flash-8b-exp-0924'</td>\n",
       "      <td>'001'</td>\n",
       "      <td>'Gemini 1.5 Flash 8B Experimental 0924'</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1000000</td>\n",
       "      <td>8192</td>\n",
       "      <td>['generateContent', 'countTokens']</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.95</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>'models/learnlm-1.5-pro-experimental'</td>\n",
       "      <td>'001'</td>\n",
       "      <td>'LearnLM 1.5 Pro Experimental'</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32767</td>\n",
       "      <td>8192</td>\n",
       "      <td>['generateContent', 'countTokens']</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.95</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>'models/gemini-exp-1114'</td>\n",
       "      <td>'exp-1114'</td>\n",
       "      <td>'Gemini Experimental 1114'</td>\n",
       "      <td>'Experimental release (November 11th, 2024) of...</td>\n",
       "      <td>32767</td>\n",
       "      <td>8192</td>\n",
       "      <td>['generateContent', 'countTokens']</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.95</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>'models/gemini-exp-1121'</td>\n",
       "      <td>'exp-1121'</td>\n",
       "      <td>'Gemini Experimental 1121'</td>\n",
       "      <td>'Experimental release (November 21st, 2024) of...</td>\n",
       "      <td>32768</td>\n",
       "      <td>8192</td>\n",
       "      <td>['generateContent', 'countTokens']</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.95</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>'models/embedding-001'</td>\n",
       "      <td>'001'</td>\n",
       "      <td>'Embedding 001'</td>\n",
       "      <td>'Obtain a distributed representation of a text.'</td>\n",
       "      <td>2048</td>\n",
       "      <td>1</td>\n",
       "      <td>['embedContent']</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>'models/text-embedding-004'</td>\n",
       "      <td>'004'</td>\n",
       "      <td>'Text Embedding 004'</td>\n",
       "      <td>'Obtain a distributed representation of a text.'</td>\n",
       "      <td>2048</td>\n",
       "      <td>1</td>\n",
       "      <td>['embedContent']</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>'models/aqa'</td>\n",
       "      <td>'001'</td>\n",
       "      <td>'Model that performs Attributed Question Answe...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7168</td>\n",
       "      <td>1024</td>\n",
       "      <td>['generateAnswer']</td>\n",
       "      <td>0.2</td>\n",
       "      <td>None</td>\n",
       "      <td>1.0</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     name     version  \\\n",
       "0                 'models/chat-bison-001'       '001'   \n",
       "1                 'models/text-bison-001'       '001'   \n",
       "2            'models/embedding-gecko-001'       '001'   \n",
       "3          'models/gemini-1.0-pro-latest'       '001'   \n",
       "4                 'models/gemini-1.0-pro'       '001'   \n",
       "5                     'models/gemini-pro'       '001'   \n",
       "6             'models/gemini-1.0-pro-001'       '001'   \n",
       "7   'models/gemini-1.0-pro-vision-latest'       '001'   \n",
       "8              'models/gemini-pro-vision'       '001'   \n",
       "9          'models/gemini-1.5-pro-latest'       '001'   \n",
       "10            'models/gemini-1.5-pro-001'       '001'   \n",
       "11            'models/gemini-1.5-pro-002'       '002'   \n",
       "12                'models/gemini-1.5-pro'       '001'   \n",
       "13       'models/gemini-1.5-pro-exp-0801'  'exp-0801'   \n",
       "14       'models/gemini-1.5-pro-exp-0827'  'exp-0827'   \n",
       "15       'models/gemini-1.5-flash-latest'       '001'   \n",
       "16          'models/gemini-1.5-flash-001'       '001'   \n",
       "17   'models/gemini-1.5-flash-001-tuning'       '001'   \n",
       "18              'models/gemini-1.5-flash'       '001'   \n",
       "19     'models/gemini-1.5-flash-exp-0827'  'exp-0827'   \n",
       "20          'models/gemini-1.5-flash-002'       '002'   \n",
       "21           'models/gemini-1.5-flash-8b'       '001'   \n",
       "22       'models/gemini-1.5-flash-8b-001'       '001'   \n",
       "23    'models/gemini-1.5-flash-8b-latest'       '001'   \n",
       "24  'models/gemini-1.5-flash-8b-exp-0827'       '001'   \n",
       "25  'models/gemini-1.5-flash-8b-exp-0924'       '001'   \n",
       "26  'models/learnlm-1.5-pro-experimental'       '001'   \n",
       "27               'models/gemini-exp-1114'  'exp-1114'   \n",
       "28               'models/gemini-exp-1121'  'exp-1121'   \n",
       "29                 'models/embedding-001'       '001'   \n",
       "30            'models/text-embedding-004'       '004'   \n",
       "31                           'models/aqa'       '001'   \n",
       "\n",
       "                                         display_name  \\\n",
       "0                              'PaLM 2 Chat (Legacy)'   \n",
       "1                                   'PaLM 2 (Legacy)'   \n",
       "2                                   'Embedding Gecko'   \n",
       "3                             'Gemini 1.0 Pro Latest'   \n",
       "4                                    'Gemini 1.0 Pro'   \n",
       "5                                    'Gemini 1.0 Pro'   \n",
       "6                       'Gemini 1.0 Pro 001 (Tuning)'   \n",
       "7                             'Gemini 1.0 Pro Vision'   \n",
       "8                             'Gemini 1.0 Pro Vision'   \n",
       "9                             'Gemini 1.5 Pro Latest'   \n",
       "10                               'Gemini 1.5 Pro 001'   \n",
       "11                               'Gemini 1.5 Pro 002'   \n",
       "12                                   'Gemini 1.5 Pro'   \n",
       "13                 'Gemini 1.5 Pro Experimental 0801'   \n",
       "14                 'Gemini 1.5 Pro Experimental 0827'   \n",
       "15                          'Gemini 1.5 Flash Latest'   \n",
       "16                             'Gemini 1.5 Flash 001'   \n",
       "17                      'Gemini 1.5 Flash 001 Tuning'   \n",
       "18                                 'Gemini 1.5 Flash'   \n",
       "19               'Gemini 1.5 Flash Experimental 0827'   \n",
       "20                             'Gemini 1.5 Flash 002'   \n",
       "21                              'Gemini 1.5 Flash-8B'   \n",
       "22                          'Gemini 1.5 Flash-8B 001'   \n",
       "23                       'Gemini 1.5 Flash-8B Latest'   \n",
       "24            'Gemini 1.5 Flash 8B Experimental 0827'   \n",
       "25            'Gemini 1.5 Flash 8B Experimental 0924'   \n",
       "26                     'LearnLM 1.5 Pro Experimental'   \n",
       "27                         'Gemini Experimental 1114'   \n",
       "28                         'Gemini Experimental 1121'   \n",
       "29                                    'Embedding 001'   \n",
       "30                               'Text Embedding 004'   \n",
       "31  'Model that performs Attributed Question Answe...   \n",
       "\n",
       "                                          description input_token_limit  \\\n",
       "0   'A legacy text-only model optimized for chat c...              4096   \n",
       "1   'A legacy model that understands text and gene...              8196   \n",
       "2    'Obtain a distributed representation of a text.'              1024   \n",
       "3                                                 NaN             30720   \n",
       "4   'The best model for scaling across a wide rang...             30720   \n",
       "5   'The best model for scaling across a wide rang...             30720   \n",
       "6                                                 NaN             30720   \n",
       "7                                                 NaN             12288   \n",
       "8                                                 NaN             12288   \n",
       "9                                                 NaN           2000000   \n",
       "10                                                NaN           2000000   \n",
       "11                                                NaN           2000000   \n",
       "12                                                NaN           2000000   \n",
       "13                                                NaN           2000000   \n",
       "14                                                NaN           2000000   \n",
       "15                                                NaN           1000000   \n",
       "16                                                NaN           1000000   \n",
       "17                                                NaN             16384   \n",
       "18                                                NaN           1000000   \n",
       "19                                                NaN           1000000   \n",
       "20                                                NaN           1000000   \n",
       "21                                                NaN           1000000   \n",
       "22                                                NaN           1000000   \n",
       "23                                                NaN           1000000   \n",
       "24                                                NaN           1000000   \n",
       "25                                                NaN           1000000   \n",
       "26                                                NaN             32767   \n",
       "27  'Experimental release (November 11th, 2024) of...             32767   \n",
       "28  'Experimental release (November 21st, 2024) of...             32768   \n",
       "29   'Obtain a distributed representation of a text.'              2048   \n",
       "30   'Obtain a distributed representation of a text.'              2048   \n",
       "31                                                NaN              7168   \n",
       "\n",
       "   output_token_limit                       supported_generation_methods  \\\n",
       "0                1024          ['generateMessage', 'countMessageTokens']   \n",
       "1                1024  ['generateText', 'countTextTokens', 'createTun...   \n",
       "2                   1                   ['embedText', 'countTextTokens']   \n",
       "3                2048                 ['generateContent', 'countTokens']   \n",
       "4                2048                 ['generateContent', 'countTokens']   \n",
       "5                2048                 ['generateContent', 'countTokens']   \n",
       "6                2048  ['generateContent', 'countTokens', 'createTune...   \n",
       "7                4096                 ['generateContent', 'countTokens']   \n",
       "8                4096                 ['generateContent', 'countTokens']   \n",
       "9                8192                 ['generateContent', 'countTokens']   \n",
       "10               8192  ['generateContent', 'countTokens', 'createCach...   \n",
       "11               8192  ['generateContent', 'countTokens', 'createCach...   \n",
       "12               8192                 ['generateContent', 'countTokens']   \n",
       "13               8192                 ['generateContent', 'countTokens']   \n",
       "14               8192                 ['generateContent', 'countTokens']   \n",
       "15               8192                 ['generateContent', 'countTokens']   \n",
       "16               8192  ['generateContent', 'countTokens', 'createCach...   \n",
       "17               8192  ['generateContent', 'countTokens', 'createTune...   \n",
       "18               8192                 ['generateContent', 'countTokens']   \n",
       "19               8192                 ['generateContent', 'countTokens']   \n",
       "20               8192  ['generateContent', 'countTokens', 'createCach...   \n",
       "21               8192  ['createCachedContent', 'generateContent', 'co...   \n",
       "22               8192  ['createCachedContent', 'generateContent', 'co...   \n",
       "23               8192  ['createCachedContent', 'generateContent', 'co...   \n",
       "24               8192                 ['generateContent', 'countTokens']   \n",
       "25               8192                 ['generateContent', 'countTokens']   \n",
       "26               8192                 ['generateContent', 'countTokens']   \n",
       "27               8192                 ['generateContent', 'countTokens']   \n",
       "28               8192                 ['generateContent', 'countTokens']   \n",
       "29                  1                                   ['embedContent']   \n",
       "30                  1                                   ['embedContent']   \n",
       "31               1024                                 ['generateAnswer']   \n",
       "\n",
       "   temperature max_temperature top_p top_k  \n",
       "0         0.25            None  0.95    40  \n",
       "1          0.7            None  0.95    40  \n",
       "2         None            None  None  None  \n",
       "3          0.9            None   1.0  None  \n",
       "4          0.9            None   1.0  None  \n",
       "5          0.9            None   1.0  None  \n",
       "6          0.9            None   1.0  None  \n",
       "7          0.4            None   1.0    32  \n",
       "8          0.4            None   1.0    32  \n",
       "9          1.0             2.0  0.95    40  \n",
       "10         1.0             2.0  0.95    64  \n",
       "11         1.0             2.0  0.95    40  \n",
       "12         1.0             2.0  0.95    40  \n",
       "13         1.0             2.0  0.95    64  \n",
       "14         1.0             2.0  0.95    64  \n",
       "15         1.0             2.0  0.95    40  \n",
       "16         1.0             2.0  0.95    64  \n",
       "17         1.0             2.0  0.95    64  \n",
       "18         1.0             2.0  0.95    40  \n",
       "19         1.0             2.0  0.95    64  \n",
       "20         1.0             2.0  0.95    40  \n",
       "21         1.0             2.0  0.95    40  \n",
       "22         1.0             2.0  0.95    40  \n",
       "23         1.0             2.0  0.95    40  \n",
       "24         1.0             2.0  0.95    40  \n",
       "25         1.0             2.0  0.95    40  \n",
       "26         1.0             2.0  0.95    64  \n",
       "27         1.0             2.0  0.95    64  \n",
       "28         1.0             2.0  0.95    64  \n",
       "29        None            None  None  None  \n",
       "30        None            None  None  None  \n",
       "31         0.2            None   1.0    40  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieving a detailed description of the models for comparison\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "model_df = pd.DataFrame(columns = ['name',\n",
    "                                    'base_model_id',\n",
    "                                    'version',\n",
    "                                    'display_name',\n",
    "                                    'description',\n",
    "                                    'input_token_limit',\n",
    "                                    'output_token_limit',\n",
    "                                    'supported_generation_methods',\n",
    "                                    'temperature',\n",
    "                                    'max_temperature',\n",
    "                                    'top_p',\n",
    "                                    'top_k'])\n",
    "\n",
    "pattern = r\"(\\w+)\\s*=\\s*(.*?)(?=,\\s*\\w+=|\\)$)\"\n",
    "\n",
    "for model in genai.list_models():\n",
    "    model_text = str(model)\n",
    "    model_dict = {match[0]: match[1].strip() for match in re.findall(pattern, model_text)}\n",
    "    model_dict_listified = {key:[value] for key, value in model_dict.items()}\n",
    "    model_df = pd.concat([model_df,pd.DataFrame(model_dict_listified)], ignore_index= True)\n",
    "\n",
    "model_df.drop(columns = ['base_model_id'], inplace = True) # dropping as the values are null\n",
    "model_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A more detailed characteristics of the models can be found <a href='https://ai.google.dev/gemini-api/docs/models/gemini#gemini-1.5-flash'>here<a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H_YXCYIKvyZJ"
   },
   "source": [
    "### Output text generation\n",
    "\n",
    "By using the `gemini-1.5-flash` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-14T10:10:57.355439Z",
     "iopub.status.busy": "2024-11-14T10:10:57.355033Z",
     "iopub.status.idle": "2024-11-14T10:10:59.186424Z",
     "shell.execute_reply": "2024-11-14T10:10:59.185256Z",
     "shell.execute_reply.started": "2024-11-14T10:10:57.355403Z"
    },
    "id": "BV1o0PmcvyJF",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "flash_model = genai.GenerativeModel('gemini-1.5-flash')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imagine ... a giant, controlled bonfire.  Instead of wood, a nuclear reactor uses uranium ..., a special element that releases incredible amounts of heat when its atoms split apart ( ...this is called nuclear fission).\n",
      "\n",
      "This heat boils water, creating high-pressure steam.  That steam spins giant turbines, which are connected to generators.   ...These generators produce electricity, just like a power plant using coal or gas, but with a much more powerful \"bonfire.\"\n",
      "\n",
      "To keep the \"bonfire ...\" under control, the reactor uses control rods.  These rods absorb some of the neutrons that cause the uranium to split, slowing down the reaction and preventing it from becoming too intense (a meltdown).  The entire process is carefully monitored and ... managed by computers and technicians to ensure safety and efficient power generation.\n",
      "\n",
      "The used uranium fuel is highly radioactive and needs to be carefully stored for many years to allow it to become less dangerous.\n",
      " ..."
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Imagine a giant, controlled bonfire.  Instead of wood, a nuclear reactor uses uranium, a special element that releases incredible amounts of heat when its atoms split apart (this is called nuclear fission).\n",
       "\n",
       "This heat boils water, creating high-pressure steam.  That steam spins giant turbines, which are connected to generators.  These generators produce electricity, just like a power plant using coal or gas, but with a much more powerful \"bonfire.\"\n",
       "\n",
       "To keep the \"bonfire\" under control, the reactor uses control rods.  These rods absorb some of the neutrons that cause the uranium to split, slowing down the reaction and preventing it from becoming too intense (a meltdown).  The entire process is carefully monitored and managed by computers and technicians to ensure safety and efficient power generation.\n",
       "\n",
       "The used uranium fuel is highly radioactive and needs to be carefully stored for many years to allow it to become less dangerous.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Text generation, first with text stream showing partial results, second with markdown\n",
    "response = flash_model.generate_content(\"Explain how nuclear reactor works, without being too much technical\",\n",
    "                                        stream=True)\n",
    "for chunk in response:\n",
    "    print(chunk.text, end=\" \")\n",
    "    print(\"...\", end = '')\n",
    "Markdown(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-14T10:11:20.713420Z",
     "iopub.status.busy": "2024-11-14T10:11:20.713011Z",
     "iopub.status.idle": "2024-11-14T10:11:21.474276Z",
     "shell.execute_reply": "2024-11-14T10:11:21.473047Z",
     "shell.execute_reply.started": "2024-11-14T10:11:20.713382Z"
    },
    "id": "lV_S5ZL5MidD",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is it round?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Chat generation\n",
    "chat = flash_model.start_chat(history=[\n",
    "    {'role': 'user' , 'parts' : \"Try to guess a fruit, in two questions\"}\n",
    "])\n",
    "response = chat.send_message(\"I'll start with a hint : it's a tropical fruit\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is it primarily yellow or orange when ripe?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = chat.send_message('Yes')\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-14T10:11:24.090237Z",
     "iopub.status.busy": "2024-11-14T10:11:24.089806Z",
     "iopub.status.idle": "2024-11-14T10:11:25.015405Z",
     "shell.execute_reply": "2024-11-14T10:11:25.014172Z",
     "shell.execute_reply.started": "2024-11-14T10:11:24.090180Z"
    },
    "id": "7b0372c3c64a",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My guess is a mango.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = chat.send_message('Yes')\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chat.send_message('Yes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[parts {\n",
       "   text: \"Try to guess a fruit, in two questions\"\n",
       " }\n",
       " role: \"user\",\n",
       " parts {\n",
       "   text: \"I\\'ll start with a hint : it\\'s a tropical fruit\"\n",
       " }\n",
       " role: \"user\",\n",
       " parts {\n",
       "   text: \"Is it round?\\n\"\n",
       " }\n",
       " role: \"model\",\n",
       " parts {\n",
       "   text: \"Yes\"\n",
       " }\n",
       " role: \"user\",\n",
       " parts {\n",
       "   text: \"Is it primarily yellow or orange when ripe?\\n\"\n",
       " }\n",
       " role: \"model\",\n",
       " parts {\n",
       "   text: \"Yes\"\n",
       " }\n",
       " role: \"user\",\n",
       " parts {\n",
       "   text: \"My guess is a mango.\\n\"\n",
       " }\n",
       " role: \"model\",\n",
       " parts {\n",
       "   text: \"Yes\"\n",
       " }\n",
       " role: \"user\",\n",
       " parts {\n",
       "   text: \"Great!  I guessed it.\\n\"\n",
       " }\n",
       " role: \"model\"]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Show chat history\n",
    "chat.history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5rU_UBlZdooM"
   },
   "source": [
    "## Explore generation parameters\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G7NfEizeipbW"
   },
   "source": [
    "### Output length\n",
    "\n",
    "Using `max_output_tokens` parameter, and prompt engineering. Note that 1 token is about 4 characters and does not equal word counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-14T10:16:28.543913Z",
     "iopub.status.busy": "2024-11-14T10:16:28.543467Z",
     "iopub.status.idle": "2024-11-14T10:16:29.813613Z",
     "shell.execute_reply": "2024-11-14T10:16:29.812292Z",
     "shell.execute_reply.started": "2024-11-14T10:16:28.543872Z"
    },
    "id": "qVf23JsIi9ma",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "While seemingly trivial, chewing gum holds a surprisingly significant role in modern society, extending beyond simple oral hygiene.  Its function as a stress reliever is widely acknowledged; the repetitive act provides a distraction and outlet for nervous energy, particularly beneficial in high-pressure environments.  This contributes to improved focus and reduced anxiety, indirectly boosting productivity and well-being.\n",
      "\n",
      "Furthermore, chewing gum stimulates saliva production, enhancing oral health by neutralizing acids and washing away food particles.  This contributes to the prevention of cavities and gum disease, reducing the need for extensive and costly dental treatments.  The resulting fresher breath also has positive social implications, fostering confidence in social interactions.\n",
      "\n",
      "Beyond its functional benefits, the chewing gum industry contributes significantly to the economy, supporting numerous jobs in manufacturing, distribution, and marketing.  The diverse flavors and innovative packaging also contribute to a vibrant consumer landscape, stimulating creativity and appealing to a wide range of preferences.  Finally, chewing gum serves as a readily available and affordable pleasure, a small indulgence\n"
     ]
    }
   ],
   "source": [
    "short_model = genai.GenerativeModel(\n",
    "    'gemini-1.5-flash',\n",
    "    generation_config=genai.GenerationConfig(max_output_tokens=200))\n",
    "\n",
    "response = short_model.generate_content('Write a 198 word essay on the importance of chewing gum in modern society.')\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "alx-WaAvir_9"
   },
   "source": [
    "### Temperature, top-k, and top-p\n",
    "\n",
    "`temperature` parameter is used to define the degree of randomness in selecting tokens. \n",
    "\n",
    "In case of `gemini-1.5-flash` model, default value is set at 1.0, with 2.0\tas the possible maximum value. \n",
    "\n",
    "Higher the temperature, higher number of candidate tokens to select from, resulting in more open ended and creative results. On the other hand, lower temperature restricts the number of possible tokens. Temperature 0, also referred to as 'greedy decoding', selects the token with the highest probability.  \n",
    "\n",
    "To note that temperature doesn't provide any guarantees of randomness, but it can be used to somewhat \"nudge\" the output.\n",
    "\n",
    "`top-K` indicates how many token options the model is going to consider. Default value is 40. 1 means choosing the most probable token (greedy decoding). \n",
    "\n",
    "`top-P` indicates the cumulative sum threshold of the tokens' probabilities from the most to least probable. Default value is 0.95. Higher value means more random responses. \n",
    "\n",
    "The order of execution in Gemini API is `top-K` -> `top-P` -> `temperature`\n",
    "\n",
    "Now let's get random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-14T10:17:03.277337Z",
     "iopub.status.busy": "2024-11-14T10:17:03.276244Z",
     "iopub.status.idle": "2024-11-14T10:17:05.103006Z",
     "shell.execute_reply": "2024-11-14T10:17:05.101366Z",
     "shell.execute_reply.started": "2024-11-14T10:17:03.277286Z"
    },
    "id": "SHraGMzqnZqt",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening line from the book L'Étranger by Albert Camus :       \n",
      " Aujourd'hui, maman est morte.\n",
      " -------------------------\n",
      "Greedy model : top-K 1, top_p 0.1, temperature 0\n",
      "\n",
      "Winston Smith, his chin nuzzled into his threadbare collar, hurried through the streets of London.  \n",
      "A telescreen's watchful eye seemed to follow him from every shadowed alleyway.\n",
      " -------------------------\n",
      "Default model : top-K 40, top_p 0.95, temperature 1\n",
      "\n",
      "Winston Smith, his chin nuzzled deep in his threadbare collar, hurried through the glass doors of the Ministry of Truth.  \n",
      "A telescreen, its ever-watchful eye gleaming, seemed to follow his every move.\n",
      " -------------------------\n",
      "Random model : top-K 5000, top_p 1, temperature 2\n",
      "\n",
      "Winston Smith, his chin nuzzled into his threadbare collar, hurried through the streets.\n",
      "\n",
      "\n",
      "The telescreen's unsettling gaze seemed to follow him, a constant, vigilant intrusion.\n",
      " -------------------------\n"
     ]
    }
   ],
   "source": [
    "from google.api_core import retry\n",
    "\n",
    "# When running lots of queries, it's a good practice to use a retry policy so your code\n",
    "# automatically retries when hitting Resource Exhausted (quota limit) errors.\n",
    "retry_policy = {\n",
    "    \"retry\": retry.Retry(predicate=retry.if_transient_error, initial=10, multiplier=1.5, timeout=300)\n",
    "}\n",
    "\n",
    "instruction = \"\"\"Continue the story and add two sentences. New line for each sentence :\n",
    "                It was a bright cold day in April, and the clocks were striking thirteen.\"\"\"\n",
    "\n",
    "print(f\"Opening line from the book L'Étranger by Albert Camus : \\\n",
    "      \\n Aujourd'hui, maman est morte.\\n {'-'*25}\")\n",
    "\n",
    "params = [['greedy', 1, 0.1, 0], # name, top-K, top-P, temperature\n",
    "          ['default', 40, 0.95, 1],\n",
    "          ['random', 5000, 1, 2]]\n",
    "\n",
    "for name, top_k, top_p, temp in params:\n",
    "    model = genai.GenerativeModel('gemini-1.5-flash',\n",
    "                                  generation_config=genai.GenerationConfig(temperature=temp,\n",
    "                                                                           top_p=top_p,\n",
    "                                                                           top_k=top_k,\n",
    "                                                                           max_output_tokens=200))\n",
    "    print(f'{name.capitalize()} model : top-K {top_k}, top_p {top_p}, temperature {temp}\\n')\n",
    "    response = model.generate_content(instruction, request_options=retry_policy)\n",
    "\n",
    "    if response.parts:\n",
    "        print(response.text, '-' * 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z3J4pCTuof7e"
   },
   "source": [
    "Interesting how the sentences begins with the same phrase \"Winston Smith, his chin nuzzled into his threadbare collar, hurried\", no matter the value of the parameters. The following sentences also show that the model is cognisant of the exerpt from 1984.  \n",
    "\n",
    "The actual phrases in the book is : <i>Winston Smith, his chin nuzzled into his breast in an effort to escape the vile wind, slipped quickly through the glass doors of Victory Mansions, though not quickly enough to prevent a swirl of gritty dust from entering along with him.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rMrYs1koY6DX"
   },
   "source": [
    "## Prompting techniques\n",
    "\n",
    "- Zero shot (General Prompting) : Providing a input directly (i.e. question, instruction) <i>without examples</i>\n",
    "\n",
    "- One-shot : Provide one example that the model can use to imitate to complete the task \n",
    "\n",
    "- Few shot : Provide multiple examples to show the model a pattern to follow. Compared to one-shot, this approach increases the chance the model follows the pattern. Rule of thumb is to provide at least three to five examples.\n",
    "\n",
    "Different prompting approaches \n",
    "\n",
    "- System prompting : Set overall context and and purpose. Define the model's fundamental capabilities and overarching purpose (also to avoid hallucinating).\n",
    "\n",
    "- Contextual prompting: Provide specific details or backgrounds relevant to the task.\n",
    "\n",
    "- Role prompting : Assign a specific character or identity. Frame the model's output style and voice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4a93e338e57c"
   },
   "source": [
    "### Chain of Thought (CoT)\n",
    "\n",
    "Prompting technique for improving the model's reasoning capabilities by generating intermediate reasoning steps. As a result, the model generates more accurate answers, less prone to hallucination \n",
    "\n",
    "It typically gets better results, especially when combined with few-shot examples, but it also tends to cost more to run, due to the increased token count.\n",
    "\n",
    "Based on the concept of CoT, Tree of Thoughts (ToT) utilizes multiple different reasoning paths simultaneously to obtain the output. \n",
    "\n",
    "### Self-consistency\n",
    "\n",
    "While CoT uses greedy decoding strategy, self-consistency prompting improves reasoning and accuracy by generating multiple responses to the same prompt and then aggregates the responses and select the most consistent outcome.\n",
    "\n",
    "### ReAct (reason & act)\n",
    "\n",
    "ReAct combines external tools with the model's reasoning, allowing the model to perform actions, including interacting with external APIs to retrieve information. ReAct prompting works by combining reasoning and acting into a thought-action loop.\n",
    "\n",
    "### Automatic Prompt Engineering\n",
    "\n",
    "Prompting a model to generate more prompts, evaluate and repeat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Suggested template for documenting prompts\n",
    "- Name\n",
    "- Goal\n",
    "- Model\n",
    "- Temperature\n",
    "- Token limit\n",
    "- Top-K\n",
    "- Top-P\n",
    "- Prompt\n",
    "- Output"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "day-1-prompting.ipynb",
   "toc_visible": true
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
